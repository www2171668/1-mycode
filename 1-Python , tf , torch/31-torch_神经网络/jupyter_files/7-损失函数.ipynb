{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy cost is 6.000000\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,1], [3,4,1]])\n",
    "B = np.array([[1,4,1], [2,3,1]])\n",
    "\n",
    "cost_np = np.sum((A-B)**2)\n",
    "print('Numpy cost is %f' % cost_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （一）适用于回归问题的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = torch.FloatTensor(A)   # 将numpy数据转换为torch数据\n",
    "B = torch.FloatTensor(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch cost is  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 1、mse loss: measures the mean squared error (squared L2 norm) between each element in the input x and target y.\n",
    "cost_torch = F.mse_loss(A, B) # reduction: ‘none’ | ‘mean’ | ‘sum’   在最后做了均值处理\n",
    "print('Torch cost is ', cost_torch)   # cost_torch = cost_np / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch cost is  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 2、l1_loss: measures the mean absolute error (MAE) between each element in the input x and target y.\n",
    "cost_torch = F.l1_loss(A, B, reduction='sum')\n",
    "print('Torch cost is ', cost_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （二）适用于分类问题的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986],\n",
      "        [ 0.4033,  0.8380, -0.7193]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 多分类用的交叉熵损失函数\n",
    "torch.manual_seed(0)   # 设置种子数  ★\n",
    "predict = torch.randn([3,3], requires_grad=True) # 3个样本，每个样本可以属于3类\n",
    "target = torch.tensor([1, 0, 2]) # 真实label，第一个样本属于第1类，第二个样本属于第0类，...\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—— F.softmax() + torch.log() = F.log_softmax()\n",
    "\n",
    "—— F.nll_loss：把经过log_softmax函数的值与标签（Label）对应的那个值拿出来相加求和，再求均值，最后添加负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax tensor([[0.8446, 0.1349, 0.0205],\n",
      "        [0.7511, 0.1438, 0.1051],\n",
      "        [0.3484, 0.5382, 0.1134]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "tensor([[-0.1689, -2.0033, -3.8886],\n",
      "        [-0.2862, -1.9392, -2.2532],\n",
      "        [-1.0543, -0.6196, -2.1769]], grad_fn=<LogBackward>)\n",
      "\n",
      "nll loss tensor(1.4888, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\4_IT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 1、分两步来计算CE loss，即先softmax，然后调用the negative log likelihood loss\n",
    "temp = F.softmax(predict)\n",
    "print('softmax', temp)   # 归一化，把预测值作为概率看待\n",
    "print()\n",
    "\n",
    "temp = torch.log(temp)\n",
    "print(temp)\n",
    "print()\n",
    "\n",
    "output = F.nll_loss(temp, target)\n",
    "print(\"nll loss\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—— F.cross_entropy(﹡,﹡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss tensor(1.4888, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 2、直接调用Cross_entropy损失函数\n",
    "output = F.cross_entropy(predict, target)\n",
    "print(\"cross entropy loss\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
