{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1、生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F   # 引入激活函数方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—— unsqueeze(﹡,dim) 增加维度 ★\n",
    "\n",
    "    dim=0时，在第0维上增加维度，将数据转换为[1,-1]类型\n",
    "    dim=1时，在第1维上增加维度，将数据转换为[-1,1]类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.linspace(-1,1,100)   # 一行数据\n",
    "x = torch.unsqueeze(data, dim=1)   # 转换为一列数据\n",
    "y = x.pow(2) \n",
    "print(data.size(),x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x.data, y.data, s=10, cmap='autumn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2、构建网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—— torch.nn.Module：深度学习的基类\n",
    "    \n",
    "＿init＿(self, n_features, n_hidden, n_output)\n",
    "        \n",
    "        n_features：输入层神经元数（维度）\n",
    "        n_hidden：隐层神经元数\n",
    "        n_output：输出神经元数\n",
    "        \n",
    "—— torch.nn.Linear：线性映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "    # 构造方法\n",
    "    def __init__(self, n_features, n_hidden, n_output): # 单隐含层网络，传入（1，10，1）\n",
    "        super(SimpleNet, self).__init__()   # 对父类初始化\n",
    "        self.hidden = torch.nn.Linear(n_features, n_hidden)   # 输入层->隐藏层  注意self.\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # 隐藏层->输出层\n",
    "    \n",
    "    # forward相当于对Module中的forward函数进行重构 ★\n",
    "    def forward(self, x):\n",
    "        hidden_result = self.hidden(x)   # 输入数据\n",
    "        x = F.relu(hidden_result)   # ReLu激活函数\n",
    "        x = self.predict(x)   # 输出数据\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3、设定网络参数\n",
    "—— torch.optim.SGD：SGD梯度下降\n",
    "\n",
    "    lr：学习率\n",
    "\n",
    "—— torch.nn.MSELoss():MSE损失计算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of SimpleNet(\n",
      "  (hidden): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "mynet = SimpleNet(1, 10, 1)   # 实例化类，输入网络参数。有时也可以直接在网络结构的构造方法中直接指定网络参数，这样实例化时就用()即可\n",
    "print(mynet.parameters)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()   # 计算损失值\n",
    "optimizer = torch.optim.SGD(mynet.parameters(), lr=0.1)   # optimizer获取所有parameters的引用，每个parameter都包含梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4、模型训练\n",
    "\n",
    "—— optimizer.zero_grad()：梯度清零\n",
    "\n",
    "—— optimizer.step()等同于：\n",
    "\n",
    "    linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "    linear.bias.data.sub_(0.01 * linear.bias.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 与 optimizer的关联\n",
    "1、梯度：\n",
    "\n",
    "①、FP过程：对prediction和y之间进行比对（熵或者其他loss function），产生最初的梯度\n",
    "\n",
    "②、BP过程：loss.backward()获得所有parameter的gradient，反向传播到整个网络的所有链路和节点\n",
    "\n",
    "2、参数更新：\n",
    "\n",
    "optimizer存了parameter的指针，step()根据这些parameter的gradient对parameter的值进行更新\n",
    "\n",
    "这里不需要传入梯度，因为梯度的引用已经在其构造函数中传入的mynet.parameters()中包含了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n",
      "tensor(0.0006)\n",
      "tensor(0.0006)\n",
      "tensor(0.0005)\n",
      "tensor(0.0005)\n",
      "tensor(0.0005)\n",
      "tensor(0.0004)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):   # 前后传播总次数\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize ★★\n",
    "    pred = mynet(x)   # 自动调用forward方法\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()   # 计算完一次反向梯度后，进行参数（w，b）迭代更新\n",
    "    \n",
    "    if(epoch%300 ==0): \n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5、预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9370])\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.tensor([-1.0])\n",
    "pred = mynet(test_data)   # 输出测试数据\n",
    "print(pred.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于forward的魔术方法：\n",
    "\n",
    "mynet(x)实际上等价于mynet.forward(x)\n",
    "\n",
    "等价的原因是因为 calss 中的__call__和__init__方法，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can be called like a function\n"
     ]
    }
   ],
   "source": [
    "# 例1\n",
    "class A():\n",
    "    def __call__(self):\n",
    "        print('i can be called like a function')\n",
    " \n",
    "a = A()\n",
    "a()   # 调用魔法函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can called like a function\n",
      "传入参数的类型是：<class 'int'>   值为： 1\n",
      "forward 函数被调用了\n",
      "in  forward, 传入参数类型是：<class 'int'>  值为: 2\n",
      "对象a输出的参数是： 3\n"
     ]
    }
   ],
   "source": [
    "# 例2\n",
    "class A():\n",
    "    def __call__(self, param):\n",
    "        \n",
    "        print('i can called like a function')\n",
    "        print('传入参数的类型是：{}   值为： {}'.format(type(param), param))\n",
    " \n",
    "        res = self.forward(param)   # 在__call__里调用其他的函数\n",
    "        return res+1\n",
    " \n",
    "    def forward(self, input_):\n",
    "        print('forward 函数被调用了')\n",
    "        print('in  forward, 传入参数类型是：{}  值为: {}'.format( type(input_), input_+1))\n",
    "        return input_+1   # 执行到此返回2给res，然后再+1作为类调用魔法函数的返回值\n",
    "\n",
    "a = A()   # 实例化时还是与普通实例化相同，不受影响\n",
    "output_param = a(1)   # 调用魔法函数\n",
    "print(\"对象a输出的参数是：\", output_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
